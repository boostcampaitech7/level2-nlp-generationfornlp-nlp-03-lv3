{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline(Gemma) Templete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_id = \"gg-hf/gemma-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Output\n",
    "'''\n",
    "<bos><start_of_turn>user\n",
    "Write a hello world program<end_of_turn>\n",
    "<start_of_turn>model\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAONE Templete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fa7613975241c5b580ad4cb450fbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
      "[|user|]Î£®Ïù¥ 14ÏÑ∏Îäî Ïú†Îä•Ìïú Ï°∞Ïñ∏ÏûêÏùò ÎèÑÏõÄÏùÑ ÎßéÏù¥ Î∞õÏïòÎäîÎç∞, Îã§Ïùå Ï§ë Í∑∏Í∞Ä ÌîÑÎûëÏä§Î•º Ïû¨Ìé∏ÌïòÍ≥† Ïó¥Í∞ïÏúºÎ°ú ÎßåÎìúÎäî Îç∞ ÎèÑÏõÄÏùÑ Ï§Ä Ï°∞Ïñ∏ÏûêÎäî ÎàÑÍµ¨ÏòÄÏäµÎãàÍπå?\n",
      "[|assistant|]Î£®Ïù¥ 14ÏÑ∏Ïùò ÌÜµÏπò Í∏∞Í∞Ñ ÎèôÏïà ÌîÑÎûëÏä§Î•º Ïû¨Ìé∏ÌïòÍ≥† Í∞ïÎåÄÍµ≠ÏúºÎ°ú ÎßåÎìúÎäî Îç∞ Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ Ìïú Ï£ºÏöî Ï°∞Ïñ∏ÏûêÎäî Ïû• Î∞îÌã∞Ïä§Ìä∏ ÏΩúÎ≤†Î•¥(Jean-Baptiste Colbert)ÏòÄÏäµÎãàÎã§. ÏΩúÎ≤†Î•¥Îäî Ïû¨Î¨¥Ïû•Í¥ÄÏúºÎ°úÏÑú Í≤ΩÏ†ú Ï†ïÏ±ÖÏùÑ Ï£ºÎèÑÌïòÎ©∞ Ï§ëÏÉÅÏ£ºÏùòÎ•º Ï∂îÏßÑÌñàÍ≥†, Ïù¥Î•º ÌÜµÌï¥ ÌîÑÎûëÏä§Ïùò Í≤ΩÏ†úÏ†Å Í∏∞Î∞òÏùÑ Í∞ïÌôîÌñàÏäµÎãàÎã§. Í∑∏Ïùò ÎÖ∏Î†• ÎçïÎ∂ÑÏóê ÌîÑÎûëÏä§Îäî 17ÏÑ∏Í∏∞ ÎßêÍ≥º 18ÏÑ∏Í∏∞ Ï¥àÏóê Í±∏Ï≥ê Ïú†ÎüΩÏùò Ï£ºÏöî Í∞ïÍµ≠ Ï§ë ÌïòÎÇòÎ°ú Î∂ÄÏÉÅÌï† Ïàò ÏûàÏóàÏäµÎãàÎã§.[|endofturn|]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\")\n",
    "\n",
    "# Choose your prompt\n",
    "prompt = \"Î£®Ïù¥ 14ÏÑ∏Îäî Ïú†Îä•Ìïú Ï°∞Ïñ∏ÏûêÏùò ÎèÑÏõÄÏùÑ ÎßéÏù¥ Î∞õÏïòÎäîÎç∞, Îã§Ïùå Ï§ë Í∑∏Í∞Ä ÌîÑÎûëÏä§Î•º Ïû¨Ìé∏ÌïòÍ≥† Ïó¥Í∞ïÏúºÎ°ú ÎßåÎìúÎäî Îç∞ ÎèÑÏõÄÏùÑ Ï§Ä Ï°∞Ïñ∏ÏûêÎäî ÎàÑÍµ¨ÏòÄÏäµÎãàÍπå?\"   # Korean example\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"You are EXAONE model from LG AI Research, a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "output = model.generate(\n",
    "    input_ids.to(\"cuda\"),\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "print(tokenizer.decode(output[0]))\n",
    "# prompt = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True,\n",
    "# )\n",
    "# prompt\n",
    "# '''\n",
    "# [|system|]You are EXAONE model from LG AI Research, a helpful assistant.[|endofturn|]\n",
    "# [|user|]ÎÑàÏùò ÏÜåÏõêÏùÑ ÎßêÌï¥Î¥ê\n",
    "# [|assistant|]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '[BOS]',\n",
       " 'eos_token': '[|endofturn|]',\n",
       " 'unk_token': '[UNK]',\n",
       " 'pad_token': '[PAD]'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: Tesla V100-SXM2-32GB. Max memory: 31.739 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 7.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437a4a68084d4a35a77f95e117182e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beomi/Solar-Ko-Recovery-11B does not have a padding token! Will use pad_token = <unk>.\n"
     ]
    }
   ],
   "source": [
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"beomi/Solar-Ko-Recovery-11B\",\n",
    "    max_seq_length=1024,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 523, 2521, 28730, 1009, 28730, 499, 28767, 3549], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<start_of_turn>model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_sh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
